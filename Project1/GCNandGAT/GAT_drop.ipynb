{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch_geometric as pyg\n",
    "import torch_geometric.data as pyg_data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.nn import GATConv\n",
    "import networkx as nx\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy.io\n",
    "import sys\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameters\n",
    "run_epochs = 1000\n",
    "lr = 0.01\n",
    "weight_decay = 5e-4\n",
    "graph_type = 'knn_drop'      # 'knn_drop', 'knn_nodrop', 'random1', 'random2'\n",
    "k_neighbors = 30\n",
    "knn_metric = 'cosine'      # 'cosine', 'p', 'sqeucledian'\n",
    "drop_classes = 0\n",
    "drop_edges = k_neighbors*(k_neighbors-1)//2\n",
    "drop_ratio = 1\n",
    "# 'seuclidean', 'p', 'sqeuclidean', 'mahalanobis', 'pyfunc', 'jaccard', 'nan_euclidean', 'cityblock', 'manhattan', 'precomputed', 'cosine', 'yule', 'infinity', 'sokalsneath', 'rogerstanimoto', 'euclidean', 'russellrao', 'canberra', 'haversine', 'correlation', 'l1', 'chebyshev', 'sokalmichener', 'braycurtis', 'dice', 'l2', 'hamming', 'minkowski'\n",
    "density = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed is set to 0.\n",
      "{'AD': 0, 'MCI': 1, 'MCIn': 2, 'MCIp': 3, 'NC': 4}\n",
      "Shape of data matrix:  (301, 186)\n",
      "Shape of labels vector:  (301,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Calculus\\AppData\\Local\\Temp\\ipykernel_9000\\2039538187.py:206: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  dist_valid=np.array(dist_valid)\n",
      "C:\\Users\\Calculus\\AppData\\Local\\Temp\\ipykernel_9000\\2039538187.py:207: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  dist_test=np.array(dist_test)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target module GATConv(186, 4, heads=8) is not supported. Currently, only the following modules are supported: `torch.nn.Linear`, `torch.nn.Embedding`, `torch.nn.Conv2d`, `transformers.pytorch_utils.Conv1D`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32me:\\ML_Frontier\\GCNandGAT\\GAT_drop.ipynb Cell 3\u001b[0m line \u001b[0;36m5\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/ML_Frontier/GCNandGAT/GAT_drop.ipynb#W2sZmlsZQ%3D%3D?line=570'>571</a>\u001b[0m module \u001b[39m=\u001b[39m GCN(num_features, num_classes)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/ML_Frontier/GCNandGAT/GAT_drop.ipynb#W2sZmlsZQ%3D%3D?line=571'>572</a>\u001b[0m module_copy \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(module)  \u001b[39m# we keep a copy of the original model for later\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/e%3A/ML_Frontier/GCNandGAT/GAT_drop.ipynb#W2sZmlsZQ%3D%3D?line=572'>573</a>\u001b[0m peft_model \u001b[39m=\u001b[39m peft\u001b[39m.\u001b[39;49mget_peft_model(module, peft_config)\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/ML_Frontier/GCNandGAT/GAT_drop.ipynb#W2sZmlsZQ%3D%3D?line=573'>574</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(peft_model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlr)\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/ML_Frontier/GCNandGAT/GAT_drop.ipynb#W2sZmlsZQ%3D%3D?line=574'>575</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n",
      "File \u001b[1;32md:\\Python\\Python310\\lib\\site-packages\\peft\\mapping.py:113\u001b[0m, in \u001b[0;36mget_peft_model\u001b[1;34m(model, peft_config, adapter_name)\u001b[0m\n\u001b[0;32m    110\u001b[0m peft_config\u001b[39m.\u001b[39mbase_model_name_or_path \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mname_or_path\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    112\u001b[0m \u001b[39mif\u001b[39;00m peft_config\u001b[39m.\u001b[39mtask_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m MODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[39m.\u001b[39mkeys() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m peft_config\u001b[39m.\u001b[39mis_prompt_learning:\n\u001b[1;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m PeftModel(model, peft_config, adapter_name\u001b[39m=\u001b[39;49madapter_name)\n\u001b[0;32m    114\u001b[0m \u001b[39mif\u001b[39;00m peft_config\u001b[39m.\u001b[39mis_prompt_learning:\n\u001b[0;32m    115\u001b[0m     peft_config \u001b[39m=\u001b[39m _prepare_prompt_learning_config(peft_config, model_config)\n",
      "File \u001b[1;32md:\\Python\\Python310\\lib\\site-packages\\peft\\peft_model.py:121\u001b[0m, in \u001b[0;36mPeftModel.__init__\u001b[1;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_peft_config \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    120\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m PEFT_TYPE_TO_MODEL_MAPPING[peft_config\u001b[39m.\u001b[39mpeft_type]\n\u001b[1;32m--> 121\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(model, {adapter_name: peft_config}, adapter_name)\n\u001b[0;32m    122\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_additional_trainable_modules(peft_config, adapter_name)\n\u001b[0;32m    124\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model, \u001b[39m\"\u001b[39m\u001b[39mconfig\u001b[39m\u001b[39m\"\u001b[39m, {\u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mcustom\u001b[39m\u001b[39m\"\u001b[39m})\n",
      "File \u001b[1;32md:\\Python\\Python310\\lib\\site-packages\\peft\\tuners\\lora\\model.py:111\u001b[0m, in \u001b[0;36mLoraModel.__init__\u001b[1;34m(self, model, config, adapter_name)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, model, config, adapter_name) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(model, config, adapter_name)\n",
      "File \u001b[1;32md:\\Python\\Python310\\lib\\site-packages\\peft\\tuners\\tuners_utils.py:94\u001b[0m, in \u001b[0;36mBaseTuner.__init__\u001b[1;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mconfig\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     92\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mcustom\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m---> 94\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minject_adapter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, adapter_name)\n\u001b[0;32m     96\u001b[0m \u001b[39m# Copy the peft_config in the injected model.\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mpeft_config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpeft_config\n",
      "File \u001b[1;32md:\\Python\\Python310\\lib\\site-packages\\peft\\tuners\\tuners_utils.py:251\u001b[0m, in \u001b[0;36mBaseTuner.inject_adapter\u001b[1;34m(self, model, adapter_name)\u001b[0m\n\u001b[0;32m    244\u001b[0m     parent, target, target_name \u001b[39m=\u001b[39m _get_submodules(model, key)\n\u001b[0;32m    246\u001b[0m     optional_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    247\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mloaded_in_8bit\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mgetattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39mis_loaded_in_8bit\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m),\n\u001b[0;32m    248\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mloaded_in_4bit\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mgetattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39mis_loaded_in_4bit\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m),\n\u001b[0;32m    249\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcurrent_key\u001b[39m\u001b[39m\"\u001b[39m: key,\n\u001b[0;32m    250\u001b[0m     }\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_and_replace(peft_config, adapter_name, target, target_name, parent, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptional_kwargs)\n\u001b[0;32m    253\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_target_modules_in_base_model:\n\u001b[0;32m    254\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    255\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTarget modules \u001b[39m\u001b[39m{\u001b[39;00mpeft_config\u001b[39m.\u001b[39mtarget_modules\u001b[39m}\u001b[39;00m\u001b[39m not found in the base model. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    256\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPlease check the target modules and try again.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    257\u001b[0m     )\n",
      "File \u001b[1;32md:\\Python\\Python310\\lib\\site-packages\\peft\\tuners\\lora\\model.py:193\u001b[0m, in \u001b[0;36mLoraModel._create_and_replace\u001b[1;34m(self, lora_config, adapter_name, target, target_name, parent, current_key, **optional_kwargs)\u001b[0m\n\u001b[0;32m    185\u001b[0m     target\u001b[39m.\u001b[39mupdate_layer(\n\u001b[0;32m    186\u001b[0m         adapter_name,\n\u001b[0;32m    187\u001b[0m         r,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    190\u001b[0m         lora_config\u001b[39m.\u001b[39minit_lora_weights,\n\u001b[0;32m    191\u001b[0m     )\n\u001b[0;32m    192\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 193\u001b[0m     new_module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_new_module(lora_config, adapter_name, target, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    194\u001b[0m     \u001b[39mif\u001b[39;00m adapter_name \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactive_adapter:\n\u001b[0;32m    195\u001b[0m         \u001b[39m# adding an additional adapter: it is not automatically trainable\u001b[39;00m\n\u001b[0;32m    196\u001b[0m         new_module\u001b[39m.\u001b[39mrequires_grad_(\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Python\\Python310\\lib\\site-packages\\peft\\tuners\\lora\\model.py:317\u001b[0m, in \u001b[0;36mLoraModel._create_new_module\u001b[1;34m(lora_config, adapter_name, target, **kwargs)\u001b[0m\n\u001b[0;32m    315\u001b[0m             kwargs[\u001b[39m\"\u001b[39m\u001b[39mfan_in_fan_out\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m lora_config\u001b[39m.\u001b[39mfan_in_fan_out \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    316\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 317\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    318\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTarget module \u001b[39m\u001b[39m{\u001b[39;00mtarget\u001b[39m}\u001b[39;00m\u001b[39m is not supported. Currently, only the following modules are supported: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    319\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`torch.nn.Linear`, `torch.nn.Embedding`, `torch.nn.Conv2d`, `transformers.pytorch_utils.Conv1D`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         )\n\u001b[0;32m    321\u001b[0m     new_module \u001b[39m=\u001b[39m Linear(adapter_name, in_features, out_features, bias\u001b[39m=\u001b[39mbias, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    323\u001b[0m \u001b[39mreturn\u001b[39;00m new_module\n",
      "\u001b[1;31mValueError\u001b[0m: Target module GATConv(186, 4, heads=8) is not supported. Currently, only the following modules are supported: `torch.nn.Linear`, `torch.nn.Embedding`, `torch.nn.Conv2d`, `transformers.pytorch_utils.Conv1D`."
     ]
    }
   ],
   "source": [
    "file_paths = ['dataset/ADNI.mat', 'dataset/PPMI.mat', 'dataset/ADNI_fMRI.mat', 'dataset/FTD_fMRI.mat', 'dataset/OCD_fMRI.mat']\n",
    "methods_acc = []\n",
    "methods_std = []\n",
    "# file_paths = ['dataset/ADNI_90_120_fMRI.mat', 'dataset/FTD_90_200_fMRI.mat', 'dataset/OCD_90_200_fMRI.mat']\n",
    "\n",
    "for file_path in file_paths:\n",
    "\n",
    "    seeds=[0, 1, 2, 3, 4]\n",
    "    seeds_best_acc = []\n",
    "\n",
    "    for seed in seeds:\n",
    "\n",
    "        # 设置随机种子\n",
    "        def set_seed(seed):\n",
    "            torch.manual_seed(seed)\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "            np.random.seed(seed)\n",
    "            random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            print('Random seed is set to {}.'.format(seed))\n",
    "        set_seed(seed)\n",
    "\n",
    "\n",
    "        mat_data=scipy.io.loadmat(file_path)\n",
    "\n",
    "        train_ratio=0.6\n",
    "        valid_ratio=0.2\n",
    "        test_ratio=0.2\n",
    "\n",
    "        X_train=[]\n",
    "        X_test=[]\n",
    "        y_train=[]\n",
    "        y_test=[]\n",
    "        X_valid=[]\n",
    "        y_valid=[]\n",
    "\n",
    "        y_train_master=[] # 在主分类器中, 将 MCI, MCIn, MCIp 合并为一类的训练集标签\n",
    "        y_valid_master=[]\n",
    "        y_test_master=[]\n",
    "        Labels=[key for key in mat_data.keys() if not key.startswith('__')]\n",
    "        dict_labels = {label: i for i, label in enumerate(Labels)}\n",
    "        print(dict_labels)\n",
    "        scaler=StandardScaler()\n",
    "\n",
    "        X_train_MCI=[] # MCI, MCIn, MCIp 的训练集, 验证集及其标签\n",
    "        X_valid_MCI=[]\n",
    "        y_train_MCI=[]\n",
    "        y_valid_MCI=[]\n",
    "        X_test_MCI=[]\n",
    "        y_test_MCI=[]\n",
    "\n",
    "\n",
    "        for label, data in mat_data.items():\n",
    "            if label.startswith('__'):\n",
    "                continue\n",
    "            label_num = dict_labels[label]\n",
    "            N = np.shape(data)[0]\n",
    "            indices = np.random.permutation(N)\n",
    "            train_end = int(train_ratio * N)\n",
    "            valid_end = int((train_ratio + valid_ratio) * N)\n",
    "\n",
    "            train_index = indices[:train_end]\n",
    "            validation_index = indices[train_end:valid_end]\n",
    "            test_index = indices[valid_end:]\n",
    "            # print(indices)\n",
    "            # print(train_index,validation_index,test_index)\n",
    "            for i in range(N):\n",
    "                if i in train_index:\n",
    "                    X_train.append(data[i])\n",
    "                    y_train.append(label_num)\n",
    "                    if file_path == 'dataset/ADNI.mat' and label_num>=1 and label_num<=3:\n",
    "                        y_train_master.append(-1)\n",
    "                        X_train_MCI.append(data[i]) # MCI, MCIn, MCIp 的训练集\n",
    "                        y_train_MCI.append(label_num)\n",
    "                    else:\n",
    "                        y_train_master.append(label_num)\n",
    "                if i in validation_index:\n",
    "                    X_valid.append(data[i])\n",
    "                    y_valid.append(label_num)\n",
    "                    if file_path == 'dataset/ADNI.mat' and label_num>=1 and label_num<=3:\n",
    "                        y_valid_master.append(-1)\n",
    "                        X_valid_MCI.append(data[i]) # MCI, MCIn, MCIp 的验证集\n",
    "                        y_valid_MCI.append(label_num)\n",
    "                    else:\n",
    "                        y_valid_master.append(label_num)\n",
    "                if i in test_index:\n",
    "                    X_test.append(data[i])\n",
    "                    y_test.append(label_num)\n",
    "                    if file_path == 'dataset/ADNI.mat' and label_num>=1 and label_num<=3:\n",
    "                        y_test_master.append(-1)\n",
    "                        X_test_MCI.append(data[i]) # MCI, MCIn, MCIp 的测试集\n",
    "                        y_test_MCI.append(label_num)\n",
    "                    else:\n",
    "                        y_test_master.append(label_num)\n",
    "                    \n",
    "\n",
    "        dict_labels_MCI = {label: i for i, label in enumerate(Labels[1:])}\n",
    "        X_train = np.array(X_train)\n",
    "        y_train = np.array(y_train)\n",
    "        y_train_master = np.array(y_train_master)\n",
    "\n",
    "        X_valid = np.array(X_valid)\n",
    "        y_valid = np.array(y_valid)\n",
    "        y_valid_master = np.array(y_valid_master)\n",
    "\n",
    "        X_train_MCI = np.array(X_train_MCI)\n",
    "        y_train_MCI = np.array(y_train_MCI)\n",
    "\n",
    "        X_valid_MCI = np.array(X_valid_MCI)\n",
    "        y_valid_MCI = np.array(y_valid_MCI)\n",
    "\n",
    "        X_test = np.array(X_test)\n",
    "        y_test = np.array(y_test)\n",
    "        y_test_master = np.array(y_test_master)\n",
    "        def normalize(X):\n",
    "            if len(X) != 0:\n",
    "                scaler = StandardScaler()\n",
    "                X = scaler.fit_transform(X)\n",
    "                # X=(X-np.mean(X,axis=0))/np.std(X,axis=0)\n",
    "            return X\n",
    "\n",
    "\n",
    "\n",
    "        if file_path in ['dataset/ADNI.mat', 'dataset/PPMI.mat']:\n",
    "            X_train = normalize(X_train)\n",
    "            X_valid = normalize(X_valid)\n",
    "            X_train_MCI = normalize(X_train_MCI)\n",
    "            X_valid_MCI = normalize(X_valid_MCI)\n",
    "            X_test = normalize(X_test)\n",
    "            X_test_MCI = normalize(X_test_MCI)\n",
    "\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ### Load data\n",
    "        train_num = X_train.shape[0]\n",
    "        val_num = X_valid.shape[0]\n",
    "        test_num = X_test.shape[0]\n",
    "        if file_path in ['dataset/ADNI.mat', 'dataset/PPMI.mat']:\n",
    "            num_features = X_train.shape[1]\n",
    "        else:\n",
    "            num_features = X_train.shape[1]*X_train.shape[2]\n",
    "        num_classes = len(Labels)\n",
    "\n",
    "\n",
    "        ## Construct the graph\n",
    "        def construct_knn_drop_graph(X, y, val_idx, k=5, metric='unsupervised', ratio=0.3, drop_classes=1, density=0.1, drop_edges=24):\n",
    "            '''\n",
    "            Description: Construct the knn graph of the data.\n",
    "            Input:\n",
    "            - X: Train data and test data\n",
    "            - y: The label of train data.\n",
    "            - k: The number of nearest neighbors.\n",
    "            Return:\n",
    "                - the knn graph of the data.\n",
    "            '''\n",
    "            knn = NearestNeighbors(n_neighbors=k, metric=metric)\n",
    "            knn.fit(X)\n",
    "            _, indices = knn.kneighbors(X) # Indices of the nearest points in the population matrix.\n",
    "            adj_mat = np.zeros((len(X), len(X)))\n",
    "            for i in range(len(X)):\n",
    "                for j in range(k):\n",
    "                    if i>=val_idx[0]:\n",
    "                        drop_class_idx = random.sample(range(len(Labels)), drop_classes)\n",
    "                        if indices[i][j] < val_idx[0] and y[indices[i][j]] in drop_class_idx:\n",
    "                            continue\n",
    "                    adj_mat[i][indices[i][j]] = 1\n",
    "                    adj_mat[indices[i][j]][i] = 1\n",
    "\n",
    "            dist_valid=[]\n",
    "            dist_test=[]\n",
    "            for i in range(len(X)):  # 尝试一些删边手段，修改邻接矩阵\n",
    "                for j in range(i+1,len(X)):\n",
    "                    if i >= val_idx[0] and i<test_idx[0] and j >= val_idx[0] and j<test_idx[0]: # 验证集内部进行删边\n",
    "                        if adj_mat[i][j]==1:\n",
    "                            # print(np.linalg.norm(np.squeeze(X[i])-np.squeeze(X[j])))\n",
    "                            dist_valid.append([np.linalg.norm(X[i]-X[j]),[i,j]])\n",
    "                    if i>=test_idx[0] and j>=test_idx[0]: # 测试集内部进行删边\n",
    "                        if adj_mat[i][j]==1:\n",
    "                            dist_test.append([np.linalg.norm(X[i]-X[j]),[i,j]])\n",
    "                        # 测试集内部进行删边\n",
    "                        # drop_edge_idx_test = random.sample(range(val_idx[0],test_idx[0]), drop_edges)\n",
    "                        # if indices[i][j] >= test_idx[0] and y[indices[i][j]] in drop_edge_idx_test:\n",
    "                        #     continue\n",
    "\n",
    "                        # 随机选择 drop_class 个类，断开测试集和验证集中的样本与训练集中这些类的连边\n",
    "                    \n",
    "\n",
    "                        # 随机选择 k/2 个邻居连边\n",
    "                        # random_sample = random.sample(range(k), int(k*ratio))\n",
    "                        # if j in random_sample:\n",
    "                        #     adj_mat[i][indices[i][j]] = 1\n",
    "                        #     adj_mat[indices[i][j]][i] = 1\n",
    "            dist_valid.sort(key=lambda x:x[0],reverse=True)\n",
    "            dist_test.sort(key=lambda x:x[0],reverse=True) # 按照距离进行排序\n",
    "            dist_valid=np.array(dist_valid)\n",
    "            dist_test=np.array(dist_test)\n",
    "            drop_valid=dist_valid[:drop_edges]\n",
    "            drop_test=dist_test[:drop_edges]\n",
    "            for _,idx in drop_valid:\n",
    "                adj_mat[idx[0]][idx[1]]=0\n",
    "                adj_mat[idx[1]][idx[0]]=0\n",
    "            for _,idx in drop_test:\n",
    "                adj_mat[idx[0]][idx[1]]=0\n",
    "                adj_mat[idx[1]][idx[0]]=0\n",
    "            for i in val_idx:\n",
    "                for j in test_idx:\n",
    "                    adj_mat[i][j]=0\n",
    "                    adj_mat[j][i]=0\n",
    "            # 将连边加入邻接矩阵\n",
    "            edge_index = []\n",
    "            for i in range(len(X)):\n",
    "                for j in range(i+1,len(X)):\n",
    "                    # 随机断开验证集和测试集内的点与连边\n",
    "                    if adj_mat[i][j]==1: # 对于训练集中的数据，只有两个是同一类才连边\n",
    "                        # edge_index.append([i,j])\n",
    "                        if i>=train_num or j>=train_num: # 如果两个点有一个不是训练集，就直接根据邻接矩阵连边\n",
    "                            edge_index.append([i, j])\n",
    "                        if i<train_num and j<train_num and y[i]==y[j]:\n",
    "                            edge_index.append([i, j])\n",
    "                        # if i >= val_idx[0] and np.random.rand()<=density: # 对于验证集内部的点，随机断开一些连边\n",
    "                        # drop_edge_idx = random.sample(range(val_idx[0],test_idx[0]), drop_edges)\n",
    "                        # if \n",
    "            edge_index = torch.tensor(edge_index).T\n",
    "            edge_index = edge_index.to(device)\n",
    "            X = X.astype(np.float32)\n",
    "            X = torch.tensor(X).float().to(device)\n",
    "            y = torch.tensor(y).long().to(device)\n",
    "            # print(X.shape,y.shape)\n",
    "            data = pyg.data.Data(x=X, edge_index=edge_index, y=y)\n",
    "            return data\n",
    "\n",
    "\n",
    "        def construct_knn_nodrop_graph(X, y, val_idx, k=5, metric='unsupervised', ratio=0.3, drop_class=0, density=0.1, drop_edges=0):\n",
    "            '''\n",
    "            Description: Construct the knn graph of the data.\n",
    "            \n",
    "            Input:\n",
    "            - X: Train data and test data.\n",
    "            - y: The label of train data.\n",
    "            - k: The number of nearest neighbors.\n",
    "            \n",
    "            Return:\n",
    "                - the knn graph of the data.\n",
    "            '''\n",
    "            knn=NearestNeighbors(n_neighbors=k, metric=metric)\n",
    "            knn.fit(X)\n",
    "            _, indices = knn.kneighbors(X) # Indices of the nearest points in the population matrix.\n",
    "            adj_mat=np.zeros((len(X),len(X)))\n",
    "            for i in range(len(X)):\n",
    "                for j in range(k):\n",
    "                    adj_mat[i][indices[i][j]]=1\n",
    "            edge_index = []\n",
    "            for i in range(len(X_train)):\n",
    "                for j in range(len(X_train)):\n",
    "                    if j>i and adj_mat[i][j]==1:\n",
    "                        edge_index.append([i,j])\n",
    "            edge_index = torch.tensor(edge_index).T\n",
    "            edge_index = edge_index.to(device)\n",
    "            X=X.astype(np.float32)\n",
    "            X = torch.tensor(X).float().to(device)\n",
    "            y = torch.tensor(y).long().to(device)\n",
    "            # print(X.shape,y.shape)\n",
    "            data = pyg.data.Data(x=X,edge_index=edge_index,y=y)\n",
    "            return data\n",
    "\n",
    "\n",
    "\n",
    "        def construct_random1_graph(X, y, num_neighbors=5, density=0.1):\n",
    "            '''\n",
    "            Description: Construct a random graph from the data.\n",
    "            Input:\n",
    "            - X: Data.\n",
    "            - y: Labels.\n",
    "            - num_neighbors: The number of neighbors for each node.\n",
    "            - density: The edge density in the random graph.\n",
    "            Return:\n",
    "            - A random graph as a PyTorch Geometric Data object.\n",
    "            '''\n",
    "\n",
    "            num_nodes = len(X)\n",
    "            adj_mat = np.zeros((num_nodes, num_nodes))\n",
    "\n",
    "            for i in range(num_nodes):\n",
    "                for j in range(i + 1, num_nodes):\n",
    "                    if np.random.rand() < density:\n",
    "                        adj_mat[i][j] = 1\n",
    "                        adj_mat[j][i] = 1\n",
    "\n",
    "            # Randomly select 'num_neighbors' neighbors for each node\n",
    "            for i in range(num_nodes):\n",
    "                neighbors = np.where(adj_mat[i] == 1)[0]\n",
    "                if len(neighbors) > num_neighbors:\n",
    "                    random_neighbors = np.random.choice(neighbors, num_neighbors, replace=False)\n",
    "                    adj_mat[i] = 0\n",
    "                    adj_mat[i][random_neighbors] = 1\n",
    "\n",
    "            edge_index = np.where(adj_mat == 1)\n",
    "            edge_index = torch.tensor(edge_index, dtype=torch.long).to(device)\n",
    "\n",
    "            X = X.astype(np.float32)\n",
    "            X = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "            y = torch.tensor(y, dtype=torch.long).to(device)\n",
    "\n",
    "            data = pyg.data.Data(x=X, edge_index=edge_index, y=y)\n",
    "\n",
    "            return data\n",
    "\n",
    "\n",
    "        def construct_random2_graph(X, y, num_neighbors=5, prob_rewire=0.2):\n",
    "            '''\n",
    "            Description: Construct a random graph from the data.\n",
    "            Input:\n",
    "            - X: Data.\n",
    "            - y: Labels.\n",
    "            - num_neighbors: The number of neighbors for each node.\n",
    "            - prob_rewire: The probability of rewiring edges in the Watts-Strogatz model.\n",
    "            Return:\n",
    "            - A random graph as a PyTorch Geometric Data object.\n",
    "            '''\n",
    "\n",
    "            # Create a Watts-Strogatz random graph\n",
    "            G = nx.watts_strogatz_graph(len(X), num_neighbors, prob_rewire)\n",
    "\n",
    "            # Convert the NetworkX graph to a PyTorch Geometric Data object\n",
    "            edge_index = torch.tensor(list(G.edges()), dtype=torch.long).t()\n",
    "            edge_index = edge_index.to(device)\n",
    "\n",
    "            X = X.astype(np.float32)\n",
    "            X = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "            y = torch.tensor(y, dtype=torch.long).to(device)\n",
    "\n",
    "            data = pyg_data.Data(x=X, edge_index=edge_index, y=y)\n",
    "\n",
    "            return data\n",
    "\n",
    "# class FeedbackModel(nn.Module):\n",
    "#     def __init__(self, embedding_matrix):\n",
    "        \n",
    "#         super(FeedbackModel, self).__init__()\n",
    "\n",
    "#         self.embed = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float), freeze=False)\n",
    "#         # GCNConv SAGEConv ResGatedGraphConv GraphConv(300, 128) \n",
    "#         # TransformerConv GATv2Conv GATConv(300, 128, heads=4) ChebConv(300, 128, K=2)\n",
    "#         # GCNConv SAGEConv ResGatedGraphConv GraphConv(128, 64) \n",
    "#         # TransformerConv  GATv2Conv GATConv(4*128, 64) ChebConv(128, 64, K=2)\n",
    "# #         self.gru = nn.GRU(256, 256, num_layers=1, \n",
    "# #                           dropout=0, batch_first=True,\n",
    "# #                           bidirectional=False)          # RNN, GRU\n",
    "#         # output: (N, L, D∗Hout), D = 2 if bidirectional=True otherwise 1\n",
    "#         # h_n: (D∗num_layers, N, Hout)\n",
    "#         self.gc1   = GATv2Conv(300, 128)\n",
    "#         self.pool1 = pyg_nn.TopKPooling(128, ratio=0.8)\n",
    "#         self.gc2   = GCNConv(128, 128)\n",
    "#         self.pool2 = pyg_nn.TopKPooling(128, ratio=0.8)\n",
    "#         self.lin1  = nn.Linear(256, 64)\n",
    "#         self.lin2  = nn.Linear(64, 6)\n",
    "\n",
    "#     def forward(self, data):\n",
    "#         x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "#         x = x.squeeze(1)\n",
    "#         x = self.embed(x)\n",
    "        \n",
    "#         x = F.relu(self.gc1(x, edge_index))\n",
    "#         x, edge_index, edge_attr, batch, perm, score = self.pool1(x, edge_index, None, batch)\n",
    "#         x1 = torch.cat([pyg_nn.global_max_pool(x, batch), pyg_nn.global_mean_pool(x, batch)], dim=1)\n",
    "\n",
    "#         x = F.relu(self.gc2(x, edge_index))\n",
    "#         x, edge_index, edge_attr, batch, perm, score = self.pool2(x, edge_index, None, batch)\n",
    "#         x2 = torch.cat([pyg_nn.global_max_pool(x, batch), pyg_nn.global_mean_pool(x, batch)], dim=1)\n",
    "\n",
    "#         x = x1 + x2\n",
    "#         # x, hn = self.gru(x, None)\n",
    "#         x = F.relu(self.lin1(x))\n",
    "#         x = F.dropout(x, p=0.5, training=self.training)\n",
    "#         output = F.relu(self.lin2(x))\n",
    "\n",
    "#         return output\n",
    "\n",
    "\n",
    "        ### Define the model\n",
    "        class GCN(nn.Module):\n",
    "            def __init__(self, num_node_features, num_classes):\n",
    "                super().__init__()\n",
    "\n",
    "                self.num_classes = num_classes\n",
    "\n",
    "                # self.conv1 = GCNConv(num_node_features, 64)\n",
    "                # self.conv2 = GCNConv(64, num_classes)\n",
    "                # self.norm = torch.nn.BatchNorm1d(64)\n",
    "                self.conv1 = GATConv(num_node_features, 4, heads=8, dropout=0.5)\n",
    "                self.conv2 = GATConv(4*8, num_classes, heads=8, concat=False, dropout=0.5)\n",
    "                self.norm = torch.nn.BatchNorm1d(4*8)\n",
    "\n",
    "            def forward(self, data):\n",
    "                x, edge_index = data.x, data.edge_index\n",
    "\n",
    "                x = self.conv1(x, edge_index)\n",
    "                x = self.norm(x)\n",
    "                x = F.relu(x)\n",
    "                # x = F.dropout(x, training=self.training)\n",
    "                x = self.conv2(x, edge_index)\n",
    "\n",
    "                return x\n",
    "\n",
    "\n",
    "        ### Train the model\n",
    "        def train(model):\n",
    "\n",
    "            train_acc, val_acc, test_acc = 0, 0, 0\n",
    "            best_train_acc, best_val_acc, best_test_acc = 0, 0, 0\n",
    "            best_test_epoch = 0\n",
    "            avg_test_acc = 0\n",
    "\n",
    "            # track accuracies over epochs\n",
    "            train_accuracies = []\n",
    "            val_accuracies = []\n",
    "            test_accuracies = []\n",
    "\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "            loss_function = torch.nn.CrossEntropyLoss().to(device)\n",
    "            for epoch in range(run_epochs):\n",
    "                model.train()\n",
    "                out = model(data)\n",
    "                optimizer.zero_grad()\n",
    "                # print(data.train_mask.shape, data.train_mask.shape)\n",
    "                # print(out.shape,data.y.shape)\n",
    "                # print(data.y[data.train_mask].shape)\n",
    "                # print(out[data.train_mask].shape)\n",
    "                loss = loss_function(out[data.train_mask], data.y[data.train_mask])\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_acc, val_acc, test_acc = test()\n",
    "                train_accuracies.append(train_acc)\n",
    "                val_accuracies.append(val_acc)\n",
    "                test_accuracies.append(test_acc)\n",
    "                if test_acc > best_test_acc:\n",
    "                    best_train_acc = train_acc\n",
    "                    best_val_acc = val_acc\n",
    "                    best_test_acc = test_acc\n",
    "                    best_test_epoch = epoch\n",
    "\n",
    "\n",
    "            avg_test_acc = sum(test_accuracies) / run_epochs\n",
    "            seeds_best_acc.append(best_test_acc)\n",
    "\n",
    "            # print accuracies of best epoch and the following 9 epochs\n",
    "            for i in range(best_test_epoch, min(run_epochs, best_test_epoch + 10)):\n",
    "                print(f'Epoch {i+1}: Train: {train_accuracies[i]:.4f}, Val: {val_accuracies[i]:.4f}, Test: {test_accuracies[i]:.4f}')\n",
    "\n",
    "            print(f\"Best test epoch {best_test_epoch+1}: train_acc={best_train_acc:.4f}, val_acc={best_val_acc:.4f}, test_acc={best_test_acc:.4f}\")\n",
    "            print(f\"avg_acc: {avg_test_acc:.4f}\")\n",
    "\n",
    "            plt_epoch = min(best_test_epoch+9, run_epochs)\n",
    "\n",
    "            # plot from first epoch to the best epoch\n",
    "            plt.plot(range(1, plt_epoch+1), train_accuracies[:plt_epoch], label='Train Accuracy')\n",
    "            plt.plot(range(1, plt_epoch+1), val_accuracies[:plt_epoch], label='Validation Accuracy')\n",
    "            plt.plot(range(1, plt_epoch+1), test_accuracies[:plt_epoch], label='Test Accuracy')\n",
    "            plt.ylim([min(train_accuracies[:plt_epoch]+val_accuracies[:plt_epoch]+test_accuracies[:plt_epoch]), \n",
    "                    max(train_accuracies[:plt_epoch]+val_accuracies[:plt_epoch]+test_accuracies[:plt_epoch])]) \n",
    "            # plt.plot(range(1, run_epochs+1), train_accuracies[:], label='Train Accuracy')\n",
    "            # plt.plot(range(1, run_epochs+1), val_accuracies[:], label='Validation Accuracy')\n",
    "            # plt.plot(range(1, run_epochs+1), test_accuracies[:], label='Test Accuracy')\n",
    "            # plt.ylim([min(train_accuracies[:]+val_accuracies[:]+test_accuracies[:]), \n",
    "            #         max(train_accuracies[:]+val_accuracies[:]+test_accuracies[:])]) \n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "\n",
    "        ### Test the model\n",
    "        def test():\n",
    "            model.eval()\n",
    "            log, accs = model(data), []\n",
    "            for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "                pred = log[mask].max(1)[1]\n",
    "                acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "                accs.append(acc)\n",
    "            return accs\n",
    "\n",
    "\n",
    "        model = GCN(num_features, num_classes).to(device)\n",
    "\n",
    "        ### Load and split the data\n",
    "        if file_path in ['dataset/ADNI.mat', 'dataset/PPMI.mat']:\n",
    "            X = np.concatenate((X_train, X_valid, X_test), axis=0)\n",
    "            y = np.concatenate((y_train, y_valid, y_test), axis=0)\n",
    "        else:\n",
    "            X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "            X_valid = X_valid.reshape(X_valid.shape[0], -1)\n",
    "            X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "            X = np.concatenate((X_train, X_valid, X_test), axis=0)\n",
    "            y = np.concatenate((y_train, y_valid, y_test), axis=0)\n",
    "\n",
    "        train_idx = np.array(range(X_train.shape[0]))\n",
    "        val_idx = np.array(range(X_train.shape[0], X_train.shape[0]+X_valid.shape[0]))\n",
    "        train_and_val_idx = np.array(range(X_train.shape[0]+X_valid.shape[0]))\n",
    "        test_idx = np.array(range(X_train.shape[0]+X_valid.shape[0], X_train.shape[0]+X_valid.shape[0]+X_test.shape[0]))\n",
    "        print(\"Shape of data matrix: \", X.shape)\n",
    "        print(\"Shape of labels vector: \", y.shape)\n",
    "        all_f = np.zeros((X.shape[0],), dtype=np.bool_)\n",
    "\n",
    "        if graph_type == 'knn_drop':\n",
    "            data = construct_knn_drop_graph(X, y, val_idx=val_idx, k=k_neighbors, metric=knn_metric, ratio=drop_ratio, drop_classes=drop_classes, density=density, drop_edges=drop_edges)\n",
    "        elif graph_type == 'knn_nodrop':\n",
    "            data = construct_knn_nodrop_graph(X, y, val_idx=val_idx, k=k_neighbors, metric=knn_metric, ratio=drop_ratio, density=density)\n",
    "        elif graph_type == 'random1':\n",
    "            data = construct_random1_graph(X, y, val_idx=val_idx, num_neighbors=k_neighbors, density=density)\n",
    "        elif graph_type == 'random2':\n",
    "            data = construct_random2_graph(X, y, val_idx=val_idx, num_neighbors=k_neighbors, prob_rewire=density)\n",
    "\n",
    "        all_f_tmp = all_f.copy()\n",
    "        all_f_tmp[train_idx] = True\n",
    "        train_mask = all_f_tmp\n",
    "\n",
    "        all_f_tmp = all_f.copy()\n",
    "        all_f_tmp[val_idx] = True\n",
    "        val_mask = all_f_tmp\n",
    "\n",
    "        all_f_tmp = all_f.copy()\n",
    "        all_f_tmp[train_and_val_idx] = True\n",
    "        train_and_val_mask = all_f_tmp\n",
    "\n",
    "        all_f_tmp = all_f.copy()\n",
    "        all_f_tmp[test_idx] = True\n",
    "        test_mask = all_f_tmp\n",
    "\n",
    "        # print(y.shape, train_idx.shape, val_idx.shape, train_and_val_idx.shape, test_idx.shape)\n",
    "        # print(train_mask.shape, val_mask.shape, train_and_val_mask.shape, test_mask.shape)\n",
    "        # print(y[train_mask].shape, y[val_mask].shape, y[train_and_val_mask].shape, y[test_mask].shape)\n",
    "\n",
    "        data.train_mask = train_mask\n",
    "        data.val_mask = val_mask\n",
    "        data.train_and_val_mask = train_and_val_mask\n",
    "        data.test_mask = test_mask\n",
    "\n",
    "        ### Train the model and test the model\n",
    "        # train()\n",
    "        # ('', <class '__main__.GCN'>)\n",
    "        # ('conv1', <class 'torch_geometric.nn.conv.gat_conv.GATConv'>)\n",
    "        # ('conv1.aggr_module', <class 'torch_geometric.nn.aggr.basic.SumAggregation'>)\n",
    "        # ('conv1.lin_src', <class 'torch_geometric.nn.dense.linear.Linear'>)\n",
    "        # ('conv2', <class 'torch_geometric.nn.conv.gat_conv.GATConv'>)\n",
    "        # ('conv2.aggr_module', <class 'torch_geometric.nn.aggr.basic.SumAggregation'>)\n",
    "        # ('conv2.lin_src', <class 'torch_geometric.nn.dense.linear.Linear'>)\n",
    "        # ('norm', <class 'torch.nn.modules.batchnorm.BatchNorm1d'>)\n",
    "        # for x in [(n,type(m))for n,m in model.named_modules()]:\n",
    "        #     print(x)\n",
    "        import peft\n",
    "        from peft import LoraConfig\n",
    "        peft_config = LoraConfig(\n",
    "            target_modules=['conv1','conv1.lin_src', 'conv2'],\n",
    "            modules_to_save=['conv2.lin_src'],\n",
    "        )\n",
    "        import copy\n",
    "\n",
    "        module = GCN(num_features, num_classes).to(device)\n",
    "        module_copy = copy.deepcopy(module)  # we keep a copy of the original model for later\n",
    "        peft_model = peft.get_peft_model(module, peft_config)\n",
    "        optimizer = torch.optim.Adam(peft_model.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        peft_model.print_trainable_parameters()\n",
    "        \n",
    "        %time train(peft_model)\n",
    "\n",
    "for metric in ['cosine', 'p', 'sqeuclidean', 'mahalanobis', 'pyfunc', 'jaccard', 'nan_euclidean', 'cityblock', 'manhattan', 'precomputed', 'yule', 'infinity', 'sokalsneath', 'rogerstanimoto', 'euclidean', 'russellrao', 'canberra', 'haversine', 'correlation', 'l1', 'chebyshev', 'sokalmichener', 'braycurtis', 'dice', 'l2', 'hamming', 'minkowski']:\n",
    "                \n",
    "\n",
    "\n",
    "    seeds_avg_acc = sum(seeds_best_acc) / 5\n",
    "\n",
    "    print(f\"{file_path}: The average accuracy is {seeds_avg_acc:.4f}\")\n",
    "    methods_acc.append(seeds_avg_acc)\n",
    "    methods_std.append(np.std(seeds_best_acc))\n",
    "    print(\"---------------------------------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"#########################################################################################################################################\\n\")\n",
    "i = 0\n",
    "for file_path in file_paths:\n",
    "    print(f\"{file_path}: The average accuracy is {methods_acc[i]:.4f}, std is {methods_std[i]:.4f}\")\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
